{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import RobertaTokenizer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "    \"\"\"A single training/test example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 source,\n",
    "                 target,\n",
    "                 lang\n",
    "                 ):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.lang=lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(filename):\n",
    "    \"\"\"Read examples from filename.\"\"\"\n",
    "    examples=[]\n",
    "    source,target=filename.split(',')\n",
    "    lang='java'\n",
    "    if source[-1]=='s':\n",
    "        lang='c_sharp'\n",
    "    with open(source,encoding=\"utf-8\") as f1,open(target,encoding=\"utf-8\") as f2:\n",
    "        for idx, (line1,line2) in enumerate(zip(f1,f2)):\n",
    "            line1=line1.strip()\n",
    "            line2=line2.strip()\n",
    "            examples.append(\n",
    "                Example(\n",
    "                    source=line1,\n",
    "                    target=line2,\n",
    "                    lang=lang\n",
    "                        ) \n",
    "            )\n",
    "            if idx == 5:\n",
    "                break\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_samples = \"GraphCodeBERT/translation/data/train.java-cs.txt.java,GraphCodeBERT/translation/data/train.java-cs.txt.cs\"\n",
    "examples = read_examples(path_to_samples)[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GraphCodeBERT.translation.parser import DFG_python,DFG_java,DFG_ruby,DFG_go,DFG_php,DFG_javascript,DFG_csharp\n",
    "from GraphCodeBERT.translation.parser import (remove_comments_and_docstrings,\n",
    "                   tree_to_token_index,\n",
    "                   index_to_code_token,\n",
    "                   tree_to_variable_index)\n",
    "from tree_sitter import Language, Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsers={}        \n",
    "dfg_function={\n",
    "    'python':DFG_python,\n",
    "    'java':DFG_java,\n",
    "    'ruby':DFG_ruby,\n",
    "    'go':DFG_go,\n",
    "    'php':DFG_php,\n",
    "    'javascript':DFG_javascript,\n",
    "    'c_sharp':DFG_csharp,\n",
    "}\n",
    "\n",
    "for lang in dfg_function:\n",
    "    LANGUAGE = Language('GraphCodeBERT/translation/parser/my-languages.so', lang)\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGE) \n",
    "    parser = [parser,dfg_function[lang]]    \n",
    "    parsers[lang]= parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove comments, tokenize code and extract dataflow     \n",
    "def extract_dataflow(code, parser,lang):\n",
    "    #remove comments\n",
    "    try:\n",
    "        code=remove_comments_and_docstrings(code,lang)\n",
    "    except:\n",
    "        pass    \n",
    "    #obtain dataflow\n",
    "    if lang==\"php\":\n",
    "        code=\"<?php\"+code+\"?>\"    \n",
    "    try:\n",
    "        tree = parser[0].parse(bytes(code,'utf8'))    \n",
    "        root_node = tree.root_node  \n",
    "        tokens_index=tree_to_token_index(root_node)     \n",
    "        code=code.split('\\n')\n",
    "        code_tokens=[index_to_code_token(x,code) for x in tokens_index]  \n",
    "        index_to_code={}\n",
    "        for idx,(index,code) in enumerate(zip(tokens_index,code_tokens)):\n",
    "            index_to_code[index]=(idx,code)\n",
    "        # index_to_code = (  \n",
    "        # ((0, 0), (0, 6)): (0, 'public'), \n",
    "        # ((0, 7), (0, 37)): (1, 'ListSpeechSynthesisTasksResult'), \n",
    "        # ((0, 38), (0, 62)): (2, 'listSpeechSynthesisTasks'), ...\n",
    "        # )\n",
    "        try:\n",
    "            DFG,_=parser[1](root_node,index_to_code,{})\n",
    "        except:\n",
    "            DFG=[]\n",
    "        DFG=sorted(DFG,key=lambda x:x[1])\n",
    "        indexs=set()\n",
    "        for d in DFG:\n",
    "            if len(d[-1])!=0:\n",
    "                indexs.add(d[1])\n",
    "            for x in d[-1]:\n",
    "                indexs.add(x)\n",
    "        new_DFG=[]\n",
    "        for d in DFG:\n",
    "            if d[1] in indexs:\n",
    "                new_DFG.append(d)\n",
    "        print(DFG)\n",
    "        dfg=new_DFG\n",
    "    except:\n",
    "        dfg=[]\n",
    "    return code_tokens,dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 example_id,\n",
    "                 source_ids,\n",
    "                 position_idx,\n",
    "                 dfg_to_code,\n",
    "                 dfg_to_dfg,                 \n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "\n",
    "    ):\n",
    "        self.example_id = example_id\n",
    "        self.source_ids = source_ids\n",
    "        self.position_idx = position_idx\n",
    "        self.dfg_to_code = dfg_to_code\n",
    "        self.dfg_to_dfg = dfg_to_dfg\n",
    "        self.target_ids = target_ids\n",
    "        self.source_mask = source_mask\n",
    "        self.target_mask = target_mask  \n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, args,stage=None):\n",
    "    features = []\n",
    "    for example_index, example in enumerate(tqdm(examples,total=len(examples))):\n",
    "        ##extract data flow\n",
    "        code_tokens,dfg=extract_dataflow(example.source,\n",
    "                                         parsers[\"c_sharp\" if args.source_lang == \"cs\" else \"java\"],\n",
    "                                         \"c_sharp\" if args.source_lang == \"cs\" else \"java\")\n",
    "        print(code_tokens)\n",
    "        code_tokens=[tokenizer.tokenize('@ '+x)[1:] if idx!=0 else tokenizer.tokenize(x) for idx,x in enumerate(code_tokens)]\n",
    "        ori2cur_pos={}\n",
    "        ori2cur_pos[-1]=(0,0)\n",
    "        for i in range(len(code_tokens)):\n",
    "            ori2cur_pos[i]=(ori2cur_pos[i-1][1],ori2cur_pos[i-1][1]+len(code_tokens[i]))    \n",
    "        code_tokens=[y for x in code_tokens for y in x]  \n",
    "        \n",
    "        #truncating\n",
    "        code_tokens=code_tokens[:args.max_source_length-3][:512-3]\n",
    "        source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
    "        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "        position_idx = [i+tokenizer.pad_token_id + 1 for i in range(len(source_tokens))]\n",
    "        dfg=dfg[:args.max_source_length-len(source_tokens)]\n",
    "        source_tokens+=[x[0] for x in dfg]\n",
    "        position_idx+=[0 for x in dfg]\n",
    "        source_ids+=[tokenizer.unk_token_id for x in dfg]\n",
    "        padding_length=args.max_source_length-len(source_ids)\n",
    "        position_idx+=[tokenizer.pad_token_id]*padding_length\n",
    "        source_ids+=[tokenizer.pad_token_id]*padding_length      \n",
    "        source_mask = [1] * (len(source_tokens))\n",
    "        source_mask+=[0]*padding_length        \n",
    "        \n",
    "        #reindex\n",
    "        reverse_index={}\n",
    "        for idx,x in enumerate(dfg):\n",
    "            reverse_index[x[1]]=idx\n",
    "        for idx,x in enumerate(dfg):\n",
    "            dfg[idx]=x[:-1]+([reverse_index[i] for i in x[-1] if i in reverse_index],)    \n",
    "        dfg_to_dfg=[x[-1] for x in dfg]\n",
    "        dfg_to_code=[ori2cur_pos[x[1]] for x in dfg]\n",
    "        length=len([tokenizer.cls_token])\n",
    "        dfg_to_code=[(x[0]+length,x[1]+length) for x in dfg_to_code]        \n",
    "        \n",
    "\n",
    "        #target\n",
    "        if stage==\"test\":\n",
    "            target_tokens = tokenizer.tokenize(\"None\")\n",
    "        else:\n",
    "            target_tokens = tokenizer.tokenize(example.target)[:args.max_target_length-2]\n",
    "        target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
    "        target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        target_mask = [1] *len(target_ids)\n",
    "        padding_length = args.max_target_length - len(target_ids)\n",
    "        target_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "        target_mask+=[0]*padding_length   \n",
    "       \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                 example_index,\n",
    "                 source_ids,\n",
    "                 position_idx,\n",
    "                 dfg_to_code,\n",
    "                 dfg_to_dfg,\n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "            )\n",
    "        )\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({\n",
    "    'source_lang': 'java',\n",
    "    'max_source_length': 200,\n",
    "    'max_target_length': 200\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'java'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.source_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'public ListSpeechSynthesisTasksResult listSpeechSynthesisTasks(ListSpeechSynthesisTasksRequest request) {request = beforeClientExecution(request);return executeListSpeechSynthesisTasks(request);}'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0].source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013606548309326172,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 45,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6d8a7ab36e4df39595fc6dd145f2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ListSpeechSynthesisTasksResult', 1, 'comesFrom', [], []), ('listSpeechSynthesisTasks', 2, 'comesFrom', [], []), ('', 3, 'comesFrom', [], []), ('ListSpeechSynthesisTasksRequest', 5, 'comesFrom', [], []), ('request', 6, 'comesFrom', [], []), ('request', 9, 'computedFrom', ['beforeClientExecution'], [11]), ('request', 9, 'computedFrom', ['request'], [13]), ('beforeClientExecution', 11, 'comesFrom', [], []), ('request', 13, 'comesFrom', ['request'], [6]), ('executeListSpeechSynthesisTasks', 17, 'comesFrom', [], []), ('request', 19, 'comesFrom', ['request'], [9])]\n",
      "[['public'], ['ĠList', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Result'], ['Ġlist', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks'], ['Ġ'], ['Ġ('], ['ĠList', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks', 'Request'], ['Ġrequest'], ['Ġ)'], ['Ġ{'], ['Ġrequest'], ['Ġ='], ['Ġbefore', 'Client', 'Exec', 'ution'], ['Ġ('], ['Ġrequest'], ['Ġ)'], ['Ġ;'], ['Ġreturn'], ['Ġexecute', 'List', 'Spe', 'ech', 'Sy', 'nt', 'hesis', 'T', 'asks'], ['Ġ('], ['Ġrequest'], ['Ġ)'], ['Ġ;'], ['Ġ}']]\n",
      "23\n",
      "\n",
      "[('request', 6, 'comesFrom', [], []), ('request', 9, 'computedFrom', ['beforeClientExecution'], [3]), ('request', 9, 'computedFrom', ['request'], [4]), ('beforeClientExecution', 11, 'comesFrom', [], []), ('request', 13, 'comesFrom', ['request'], [0]), ('request', 19, 'comesFrom', ['request'], [2])]\n",
      "[(29, 30), (32, 33), (32, 33), (34, 38), (39, 40), (53, 54)]\n",
      "{-1: (0, 0), 0: (0, 1), 1: (1, 10), 2: (10, 18), 3: (18, 19), 4: (19, 20), 5: (20, 29), 6: (29, 30), 7: (30, 31), 8: (31, 32), 9: (32, 33), 10: (33, 34), 11: (34, 38), 12: (38, 39), 13: (39, 40), 14: (40, 41), 15: (41, 42), 16: (42, 43), 17: (43, 52), 18: (52, 53), 19: (53, 54), 20: (54, 55), 21: (55, 56), 22: (56, 57)}\n"
     ]
    }
   ],
   "source": [
    "features = convert_examples_to_features(examples, tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.InputFeatures at 0x7fb56ba910a0>,\n",
       " <__main__.InputFeatures at 0x7fb56ba911f0>,\n",
       " <__main__.InputFeatures at 0x7fb56ba91070>,\n",
       " <__main__.InputFeatures at 0x7fb56ba91040>,\n",
       " <__main__.InputFeatures at 0x7fb56ba6ca00>,\n",
       " <__main__.InputFeatures at 0x7fb56ba6ceb0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.InputFeatures at 0x7fb56ba910a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(features[0].example_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 15110, 9527, 29235, 7529, 35615, 3999, 35571, 565, 40981, 48136, 889, 29235, 7529, 35615, 3999, 35571, 565, 40981, 1437, 36, 9527, 29235, 7529, 35615, 3999, 35571, 565, 40981, 45589, 2069, 4839, 25522, 2069, 5457, 137, 47952, 46891, 15175, 36, 2069, 4839, 25606, 671, 11189, 36583, 29235, 7529, 35615, 3999, 35571, 565, 40981, 36, 2069, 4839, 25606, 35524, 2, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(features[0].source_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(features[0].position_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(30, 31), (33, 34), (33, 34), (35, 39), (40, 41), (54, 55)]\n"
     ]
    }
   ],
   "source": [
    "print(features[0].dfg_to_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [3], [4], [], [0], [2]]\n"
     ]
    }
   ],
   "source": [
    "print(features[0].dfg_to_dfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 15110, 6229, 9527, 29235, 7529, 35615, 3999, 35571, 565, 40981, 47806, 9527, 29235, 7529, 35615, 3999, 35571, 565, 40981, 1640, 36583, 29235, 7529, 35615, 3999, 35571, 565, 40981, 45589, 2069, 48512, 10806, 1735, 5457, 92, 9318, 5361, 47261, 47006, 45012, 4, 45589, 40825, 1250, 254, 5457, 9527, 29235, 7529, 35615, 3999, 35571, 565, 40981, 45589, 40825, 1250, 254, 4, 49483, 131, 45012, 4, 47806, 9685, 119, 14980, 1250, 254, 5457, 9527, 29235, 7529, 35615, 3999, 35571, 565, 40981, 47806, 9685, 119, 14980, 1250, 254, 4, 49483, 131, 30921, 9318, 5361, 41552, 36583, 29235, 7529, 35615, 3999, 35571, 565, 40981, 47806, 49925, 16604, 6, 1735, 4397, 24303, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(features[0].target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(features[0].source_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(features[0].target_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking target_mask and source_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(features[0].target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(examples[0].target).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(features[0].source_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(examples[0].source).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(examples[0].source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check extract dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'public UpdateJourneyStateResult updateJourneyState(UpdateJourneyStateRequest request) {request = beforeClientExecution(request);return executeUpdateJourneyState(request);}'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input = examples[1].source\n",
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_tokens, dfg = extract_dataflow(sample_input, parsers[\"java\"], \"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['public',\n",
       " 'UpdateJourneyStateResult',\n",
       " 'updateJourneyState',\n",
       " '',\n",
       " '(',\n",
       " 'UpdateJourneyStateRequest',\n",
       " 'request',\n",
       " ')',\n",
       " '{',\n",
       " 'request',\n",
       " '=',\n",
       " 'beforeClientExecution',\n",
       " '(',\n",
       " 'request',\n",
       " ')',\n",
       " ';',\n",
       " 'return',\n",
       " 'executeUpdateJourneyState',\n",
       " '(',\n",
       " 'request',\n",
       " ')',\n",
       " ';',\n",
       " '}']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('request', 6, 'comesFrom', [], []),\n",
       " ('request', 9, 'computedFrom', ['beforeClientExecution'], [11]),\n",
       " ('request', 9, 'computedFrom', ['request'], [13]),\n",
       " ('beforeClientExecution', 11, 'comesFrom', [], []),\n",
       " ('request', 13, 'comesFrom', ['request'], [6]),\n",
       " ('request', 19, 'comesFrom', ['request'], [9])]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [tokenizer.tokenize('@ '+x)[1:] if idx!=0 else tokenizer.tokenize(x) for idx,x in enumerate(code_tokens)]\n",
    "temp=[y for x in temp for y in x]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "57 + 2 + 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "30\n",
      "24\n",
      "0\n",
      "1\n",
      "31\n",
      "7\n",
      "1\n",
      "1\n",
      "7\n",
      "1\n",
      "21\n",
      "1\n",
      "7\n",
      "1\n",
      "1\n",
      "6\n",
      "31\n",
      "1\n",
      "7\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "code_tokens\n",
    "for i in (code_tokens):\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6\n",
      "\n",
      "6\n",
      "36\n",
      "\n",
      "36\n",
      "60\n",
      "\n",
      "60\n",
      "60\n",
      "\n",
      "60\n",
      "61\n",
      "\n",
      "61\n",
      "92\n",
      "\n",
      "92\n",
      "99\n",
      "\n",
      "99\n",
      "100\n",
      "\n",
      "100\n",
      "101\n",
      "\n",
      "101\n",
      "108\n",
      "\n",
      "108\n",
      "109\n",
      "\n",
      "109\n",
      "130\n",
      "\n",
      "130\n",
      "131\n",
      "\n",
      "131\n",
      "138\n",
      "\n",
      "138\n",
      "139\n",
      "\n",
      "139\n",
      "140\n",
      "\n",
      "140\n",
      "146\n",
      "\n",
      "146\n",
      "177\n",
      "\n",
      "177\n",
      "178\n",
      "\n",
      "178\n",
      "185\n",
      "\n",
      "185\n",
      "186\n",
      "\n",
      "186\n",
      "187\n",
      "\n",
      "187\n",
      "188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ori2cur_pos={}\n",
    "ori2cur_pos[-1]=(0,0)\n",
    "for i in range(len(code_tokens)):\n",
    "    print(ori2cur_pos[i-1][1])\n",
    "    print(ori2cur_pos[i-1][1]+len(code_tokens[i]))\n",
    "    print()\n",
    "    ori2cur_pos[i]=(ori2cur_pos[i-1][1],ori2cur_pos[i-1][1]+len(code_tokens[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1: (0, 0),\n",
       " 0: (0, 6),\n",
       " 1: (6, 36),\n",
       " 2: (36, 60),\n",
       " 3: (60, 60),\n",
       " 4: (60, 61),\n",
       " 5: (61, 92),\n",
       " 6: (92, 99),\n",
       " 7: (99, 100),\n",
       " 8: (100, 101),\n",
       " 9: (101, 108),\n",
       " 10: (108, 109),\n",
       " 11: (109, 130),\n",
       " 12: (130, 131),\n",
       " 13: (131, 138),\n",
       " 14: (138, 139),\n",
       " 15: (139, 140),\n",
       " 16: (140, 146),\n",
       " 17: (146, 177),\n",
       " 18: (177, 178),\n",
       " 19: (178, 185),\n",
       " 20: (185, 186),\n",
       " 21: (186, 187),\n",
       " 22: (187, 188)}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori2cur_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(code_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_index={}\n",
    "temp_dfg=dfg.copy()\n",
    "for idx,x in enumerate(temp_dfg):\n",
    "    reverse_index[x[1]]=idx\n",
    "for idx,x in enumerate(temp_dfg):\n",
    "    temp_dfg[idx]=x[:-1]+([reverse_index[i] for i in x[-1] if i in reverse_index],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6: 0, 9: 2, 11: 3, 13: 4, 19: 5}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('request', 6, 'comesFrom', [], []),\n",
       " ('request', 9, 'computedFrom', ['beforeClientExecution'], [11]),\n",
       " ('request', 9, 'computedFrom', ['request'], [13]),\n",
       " ('beforeClientExecution', 11, 'comesFrom', [], []),\n",
       " ('request', 13, 'comesFrom', ['request'], [6]),\n",
       " ('request', 19, 'comesFrom', ['request'], [9])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('request', 6, 'comesFrom', [], []),\n",
       " ('request', 9, 'computedFrom', ['beforeClientExecution'], [3]),\n",
       " ('request', 9, 'computedFrom', ['request'], [4]),\n",
       " ('beforeClientExecution', 11, 'comesFrom', [], []),\n",
       " ('request', 13, 'comesFrom', ['request'], [0]),\n",
       " ('request', 19, 'comesFrom', ['request'], [2])]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg_to_dfg=[x[-1] for x in temp_dfg]\n",
    "dfg_to_code=[ori2cur_pos[x[1]] for x in temp_dfg]\n",
    "length=len([tokenizer.cls_token])\n",
    "# dfg_to_code=[(x[0]+length,x[1]+length) for x in dfg_to_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [3], [4], [], [0], [2]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg_to_dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(92, 99), (101, 108), (101, 108), (109, 130), (131, 138), (178, 185)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg_to_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(93, 100), (102, 109), (102, 109), (110, 131), (132, 139), (179, 186)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x[0]+length,x[1]+length) for x in dfg_to_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
